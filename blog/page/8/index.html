
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Gosuke Miyashita</title>
  <meta name="author" content="Gosuke Miyashita">

  
  <meta name="description" content="[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3] のつづき。今回は Device-Mapper Multipath をつかって、DRBD によりミラーされている2つのブロックデバイスを、 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://mizzy.org/blog/page/8">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Gosuke Miyashita" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-53984-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Gosuke Miyashita</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:mizzy.org" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS04/">ScalableStorageWithOSS04</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:23:16+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS04/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3] のつづき。今回は Device-Mapper Multipath をつかって、DRBD によりミラーされている2つのブロックデバイスを、仮想的にひとつに見えるようにします。</p>

<p>まずは client0 と client1 に device-mapper-multipath がインストールされていなければ、インストール。</p>

<pre><code>$ sudo yum -y install device-mapper-multipath
</code></pre>

<p>/etc/multipath.conf を修正。デフォルトでは blacklist の devnode に * が指定されていて、すべてのデバイスに対して無効になっているので、blacklist をコメントアウト。</p>

<pre><code># Blacklist all devices by default. Remove this to enable multipathing
# on the default devices.
#blacklist {
#        devnode "*"
#}
</code></pre>

<p>multipathd を restart。</p>

<pre><code>$ sudo /etc/init.d/multipathd restart
</code></pre>

<p>multipath -l で確認。</p>

<pre><code>$ sudo /sbin/multipath -l
mpath1 (1) dm-1 GNBD,GNBD
[size=1024M][features=0][hwhandler=0]
\_ round-robin 0 [prio=0][active]
 \_ #:#:#:# gnbd2 252:2 [active][undef]
 \_ #:#:#:# gnbd3 252:3 [active][undef]
mpath0 (0) dm-0 GNBD,GNBD
[size=1024M][features=0][hwhandler=0]
\_ round-robin 0 [prio=0][active]
 \_ #:#:#:# gnbd0 252:0 [active][undef]
 \_ #:#:#:# gnbd1 252:1 [active][undef]  
</code></pre>

<p>gnbd0 と gnbd1 が /dev/mapper/mpath0 として見え、gnbd2 と gnbd3 が /dev/mapper/mpath1 として見える。</p>

<p>この状態で、/dev/mapper/mpath0 へ書き込みが発生すると、gnbd0 か gnbd1 のどちらかに書き込まれ、DRBD によりもう一方にミラーされる、という状態ができあがり。図にすると以下のような状態。/dev/mapper/mpath0 から読み込んだ場合にも、gnbd0 か gnbd1 のどちらかから読み込むことになるが、DRBD によりミラーされているので、どちらを読みに行っても OK。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS04/storages_with_dmmp.jpg?format=raw)]]</p>

<p>次回は /dev/mapper/mpath0 と /dev/mapper/mpath1 を物理ボリュームとして、 CLVM によりひとつの論理ボリュームを構成し、論理ボリュームのメタデータを client0 と client1 間で共有する方法について解説します。</p>

<h1>関連エントリ</h1>

<ul>
<li>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0]</li>
<li>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1]</li>
<li>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2]</li>
<li>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3]</li>
<li>[wiki:ScalableStorageWithOSS05 OSS だけでスケーラブルなストレージを安価に構築する方法 #5]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS03/">ScalableStorageWithOSS03</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:22:39+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS03/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2] のつづき。今回はストレージサーバ側で GNBD によりブロックデバイスをエクスポートし、クライアント側でインポートするまでの手順を紹介します。</p>

<h1>ストレージクラスタ</h1>

<p>用途的にはクラスタリングする必要はないのですが、GNBD でエクスポートする際に、<a href="http://sources.redhat.com/cluster/cman/">cman(Cluster Manager)</a> を動かす必要があるため、まずは cman をインストール、設定、起動して、ストレージクラスタを構成します。</p>

<p>といっても、クラスタリングは本来の趣旨とははずれるので、設定は適当。</p>

<p>まずは storage0a と storage0b をクラスタリング。最初に storage0a で cman をインストール。</p>

<pre><code>$ sudo yum -y install cman
</code></pre>

<p>storage0a で ccs_tool を実行し、/etc/cluster/cluster.conf のスケルトンを作成。</p>

<pre><code>$ sudo /sbin/ccs_tool create storage0
</code></pre>

<p>/etc/cluster/cluster.conf を以下のように修正。</p>

<pre><code>&lt;?xml version="1.0"?&gt;
&lt;cluster name="storage0" config_version="1"&gt;
    &lt;clusternodes&gt;
        &lt;clusternode name="storage0a.example.org" nodeid="1" votes="1"/&gt;
    &lt;/clusternodes&gt;
  &lt;fencedevices/&gt;
  &lt;rm&gt;
    &lt;failoverdomains/&gt;
    &lt;resources/&gt;
  &lt;/rm&gt;
&lt;/cluster&gt;
</code></pre>

<p>cman を起動。</p>

<pre><code>$ sudo /etc/init.d/cman start
</code></pre>

<p>次に、sorage0b をクラスタに追加するべく、/etc/cluster/cluster.conf を、storage0a 上で修正。</p>

<pre><code>&lt;?xml version="1.0"?&gt;
&lt;cluster name="storage0" config_version="2"&gt;
    &lt;clusternodes&gt;
        &lt;clusternode name="storage0a.example.org" nodeid="1" votes="1"/&gt;
        &lt;clusternode name="storage0b.example.org" nodeid="2" votes="1"/&gt;
    &lt;/clusternodes&gt;
  &lt;fencedevices/&gt;
  &lt;rm&gt;
    &lt;failoverdomains/&gt;
    &lt;resources/&gt;
  &lt;/rm&gt;
&lt;/cluster&gt;
</code></pre>

<p>config_version の値をインクリメントしておくのがポイント。ccs_tool update を実行して、設定を反映。</p>

<pre><code>$ sudo /sbin/ccs_tool update /etc/cluster/cluster.conf
</code></pre>

<p>次に storage0b 側で cman を起動。</p>

<pre><code>$ sudo yum -y install cman
$ sudo /sbin/ccs_tool create storage0
$ sudo /etc/init.d/cman start
</code></pre>

<p>/etc/cluster/cluster.conf は、storage0a から自動的にコピーされる。cman_tool nodes を実行すると、クラスタメンバが表示される。</p>

<pre><code>$ sudo /sbin/cman_tool nodes
Node  Sts   Inc   Joined               Name
   1   M     24   2008-10-25 04:29:59  storage0a.example.org
   2   M      4   2008-10-25 04:23:50  storage0b.example.org
</code></pre>

<p>storage0a と storage0b のクラスタリングができたら、storage1a と storage1b を同様にクラスタリング。上記で storage0 と書かれているところを storage1 と読み替えて、同じ手順を実行。</p>

<h1>GNBD エクスポート</h1>

<p>次に、storage0a, storage0b で GNBD によりブロックデバイスをエクスポート。まずは gnbd をインストール。</p>

<pre><code>$ sudo yum -y install gnbd
</code></pre>

<p>gnbd_serv を起動。</p>

<pre><code>$ sudo /sbin/gnbd_serv
gnbd_serv: startup succeeded
</code></pre>

<p>エクスポート。storage0a では以下のように。-d で指定するデバイス名は、DRBD デバイスを指定。-e では適当なエクスポート名をつける。</p>

<pre><code>$ sudo /sbin/gnbd_export -d /dev/drbd0 -e gnbd0a -u0
gnbd_export: created GNBD gnbd0a serving file /dev/drbd0
</code></pre>

<p>storage0b では以下のように、-e オプションで指定するエクスポート名を変える。-u で指定する UID は、storage0a と storage0b で同じにする必要がある。（Device-Mapper Multipath で同一デバイスと認識させるため。）</p>

<pre><code>$ sudo /sbin/gnbd_export -d /dev/drbd0 -e gnbd0b -u0
gnbd_export: created GNBD gnbd0b serving file /dev/drbd0
</code></pre>

<p>-l オプションで、エクスポート状態を確認できる。</p>

<pre><code>$ sudo /sbin/gnbd_export -l
Server[1] : gnbd0a
--------------------------
      file : /dev/drbd0
   sectors : 2096944
  readonly : no
    cached : no
   timeout : 60
       uid : 0
</code></pre>

<p>strage1a, storage1b でも同様にエクスポートする。</p>

<h1>クライアントクラスタ</h1>

<p>GNBD でインポートするクライアント側でも cman を動かす必要があるので、client0, client1 で cman を動かす。後に出てくる CLVM でも必要。注意点は、ccs_tool create で指定するクラスタ名を、ストレージクラスタで指定したものを別にすること。</p>

<pre><code>$ sudo /sbin/ccs_tool create client  
</code></pre>

<p>あとの手順はストレージの時と一緒。</p>

<h1>GNBD インポート</h1>

<p>client0, client1 でストレージサーバからエクスポートされているデバイスをインポートする。以下の手順を、client0, client1 双方で実行。</p>

<p>まずは GNBD カーネルモジュールと GNBD をインストール。</p>

<pre><code>$ sudo yum -y install kmod-gnbd-xen gnbd
</code></pre>

<p>インポートする。</p>

<pre><code>$ sudo /sbin/modprobe gnbd
$ sudo /sbin/gnbd_import -i storage0a.example.org
$ sudo /sbin/gnbd_import -i storage0b.example.org
$ sudo /sbin/gnbd_import -i storage1a.example.org
$ sudo /sbin/gnbd_import -i storage1b.example.org
</code></pre>

<p>-l オプションでインポート状況を確認。</p>

<pre><code>$ sudo /sbin/gnbd_import -l
Device name : gnbd0a
----------------------
    Minor # : 0
 sysfs name : /block/gnbd0
     Server : storage0a.example.org
       Port : 14567
      State : Close Connected Clear
   Readonly : No
    Sectors : 2096944

Device name : gnbd0b
----------------------
    Minor # : 1
 sysfs name : /block/gnbd1
     Server : storage0b.example.org
       Port : 14567
      State : Close Connected Clear
   Readonly : No
    Sectors : 2096944

Device name : gnbd1a
----------------------
    Minor # : 2
 sysfs name : /block/gnbd2
     Server : storage1a.example.org
       Port : 14567
      State : Close Connected Clear
   Readonly : No
    Sectors : 2096944

Device name : gnbd1b
----------------------
    Minor # : 3
 sysfs name : /block/gnbd3
     Server : storage1b.example.org
       Port : 14567
      State : Close Connected Clear
   Readonly : No
    Sectors : 2096944
</code></pre>

<p>デバイスファイルを確認してみる。</p>

<pre><code>$ ls /dev/gnbd/
gnbd0a  gnbd0b  gnbd1a  gnbd1b 
</code></pre>

<p>/dev/gnbd/gnbd0a は、/dev/gnbd0 と同じデバイス。他も同様。</p>

<h1>現在の状態</h1>

<p>ここまでセットアップした状態を図にすると、以下のようになります。GNBD により、ネットワーク越しにブロックデバイスにアクセスできるようになっています。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS03/storages_with_gnbd.jpg?format=raw)]]</p>

<p>次回は DM-MP あたりを解説予定。</p>

<h1>関連エントリ</h1>

<ul>
<li>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0]</li>
<li>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1]</li>
<li>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2]</li>
<li>[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4]</li>
<li>[wiki:ScalableStorageWithOSS05 OSS だけでスケーラブルなストレージを安価に構築する方法 #5]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS02/">ScalableStorageWithOSS02</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:22:09+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS02/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1] のつづき。</p>

<h1>Xen インスタンスの作成</h1>

<p>ここは詳しく解説しません。好きなように作成してください。自分は [wiki:Cobbler Cobbler]/[wiki:Koan Koan] を使ってます。インスタンスは6つ作成。</p>

<ul>
<li>client0, client1（ストレージをマウントするインスタンス）</li>
<li>storage0a, storage0b（ストレージを構成するインスタンスセットその1）</li>
<li>storage1a, storage1b（ストレージを構成するインスタンスセットその2）</li>
</ul>


<p>全体的な構成概要については、[wiki:ScalableStorageWithOSS01 前回のエントリ] を参照してください。</p>

<h1>エクスポート用ブロックデバイスの作成</h1>

<p>storage![01][ab] 上で、GNBD によりエクスポートするブロックデバイス（ディスク）を作成。</p>

<p>ディスクイメージを作成。テスト用なので容量とかは適当に。</p>

<pre><code>$ sudo dd if=/dev/null of=/var/lib/xen/images/storage0a-disk1 bs=1M count=1 seek=1024
</code></pre>

<p>/etc/xen/storage0a の disk の項目を修正して、作成したディスクイメージを追加。</p>

<pre><code>disk = [ "file:/var/lib/xen/images/storage0a-disk0,xvda,w", "file:/var/lib/xen/images/storage0a-disk1,xvdb,w" ] 



$ sudo /usr/sbin/xm create storage0a
</code></pre>

<p>デバイスができていることを確認。</p>

<pre><code>$ dmesg|grep -B2 xvdb
Registering block device major 202
 xvda: xvda1 xvda2
 xvdb: unknown partition table 
</code></pre>

<p>パーティション作成。</p>

<pre><code>$ sudo /sbin/parted /dev/xvdb
(parted) mklabel gpt
(parted) mkpart primary 0 1074
(parted) set 1 lvm on
(parted) quit
</code></pre>

<p>これで /dev/xvdb1 が作成される。</p>

<p>storage0a 上で完了したら、storage0b, storage1a, storage1b でも同様の作業をする。</p>

<h1>DRBD の設定</h1>

<p>storage0a と storage0b の /dev/xvdb1 を同期するための設定を行う。</p>

<p>まずは必要なパッケージのインストール。(storage0a, storage0b 双方で。)</p>

<pre><code>$ sudo yum -y install kmod-drbd82-xen
</code></pre>

<p>kmod-drbd82-xen が kernel-xen-2.6.18-92.1.10.el5xen に依存していて、うちの環境ではこのバージョンのカーネルがインストールされた。で、/etc/grub.conf をいじって、2.6.18-92.1.10.el5xen で起動するようにして、再起動。</p>

<p>次に/etc/drbd.conf を設定。テストなので適当に。(storage0a, storage0b 双方で。)</p>

<pre><code>global {
    usage-count no;
}

common {
  syncer { rate 10M; }
}

resource r0 {

  protocol C;

  startup {
     become-primary-on both;
  }


  net {
     allow-two-primaries;
  }

  syncer {
    rate 10M;
  }

  on storage0a.example.org {
    device    /dev/drbd0;
    disk      /dev/xvdb1;
    address   192.168.10.20:7788;
    meta-disk internal;
  }

  on storage0b.example.org {
    device    /dev/drbd0;
    disk      /dev/xvdb1;
    address   192.168.10.27:7788;
    meta-disk internal;
  }

} 
</code></pre>

<p>設定を終えたら、起動する。(storage0a, storage0b 双方で。)</p>

<pre><code>$ sudo /sbin/modprobe drbd minor_count=1
$ sudo /sbin/drbdadm create-md r0
$ sudo /sbin/drbdadm up all
</code></pre>

<p>この時点で、どっちもセカンダリになってるはず。</p>

<pre><code>$ cat /proc/drbd
version: 8.2.6 (api:88/proto:86-88)
GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-08-07 17:50:46
 0: cs:Connected st:Secondary/Secondary ds:Inconsistent/Inconsistent C r---
    ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 oos:1048472
</code></pre>

<p>片方を primary にして、同期を開始する。(storage0a だけで実行。）</p>

<pre><code>$ sudo /sbin/drbdadm -- --overwrite-data-of-peer primary all
</code></pre>

<p>同期中は /proc/drbd がこんな感じになる。</p>

<pre><code>$ cat /proc/drbd
version: 8.2.6 (api:88/proto:86-88)
GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-08-07 17:50:46
 0: cs:SyncSource st:Primary/Secondary ds:UpToDate/Inconsistent C r---
    ns:40436 nr:0 dw:0 dr:40436 al:0 bm:2 lo:0 pe:0 ua:0 ap:0 oos:1008036
        [&gt;....................] sync'ed:  4.0% (1008036/1048472)K
        finish: 0:01:38 speed: 10,108 (10,108) K/sec
</code></pre>

<p>同期が完了するとこんな感じ。</p>

<pre><code>$ cat /proc/drbd
version: 8.2.6 (api:88/proto:86-88)
GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-08-07 17:50:46
 0: cs:Connected st:Primary/Secondary ds:UpToDate/UpToDate C r---
    ns:1048472 nr:0 dw:0 dr:1048472 al:0 bm:64 lo:0 pe:0 ua:0 ap:0 oos:0
</code></pre>

<p>両方プライマリにするので、現在のセカンダリ(storage0b)で以下を実行。</p>

<pre><code>$ sudo /sbin/drbdadm primary r0
</code></pre>

<p>両方プライマリになる。</p>

<pre><code>$ cat /proc/drbd
version: 8.2.6 (api:88/proto:86-88)
GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-08-07 17:50:46
 0: cs:Connected st:Primary/Primary ds:UpToDate/UpToDate C r---
    ns:0 nr:1048472 dw:1048472 dr:0 al:0 bm:64 lo:0 pe:0 ua:0 ap:0 oos:0
</code></pre>

<p>これで DRBD による同期は完了。sorage1a, storage1b についても同様の操作を行う。</p>

<h1>現在の状態</h1>

<p>ここまでセットアップした状態を図にすると、以下のようになります。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS02/storages_with_drbd.jpg?format=raw)]]</p>

<p>以下の [wiki:ScalableStorageWithOSS01 前回エントリの構成図] と比較してみて下さい。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS01/scalable_storage.jpg?format=raw)]]</p>

<p>次回は GNBD によるブロックデバイスのエクスポート/インポートについて説明する予定です。</p>

<h1>関連エントリ</h1>

<ul>
<li>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0]</li>
<li>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1]</li>
<li>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3]</li>
<li>[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4]</li>
<li>[wiki:ScalableStorageWithOSS05 OSS だけでスケーラブルなストレージを安価に構築する方法 #5]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS01/">ScalableStorageWithOSS01</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:21:40+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS01/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0] のつづき。</p>

<h1>サーバ構成概要</h1>

<p><a href="http://www.slideshare.net/mizzy/how-to-build-a-scalable-storage-system-at-tlug-meeting-20080913-presentation">TLUG 発表資料</a> で触れた、全体的な構成のおさらい。構成図は以下のような感じ。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS01/scalable_storage.jpg?format=raw)]]</p>

<p>この構成について説明すると、以下のようになります。</p>

<ul>
<li>DRBD により、2台一組でブロックデバイスをミラーリング

<ul>
<li>/dev/gnbd0 と /dev/gnbd1 がセット</li>
<li>/dev/gnbd2 と /dev/gnbd3 がセット</li>
</ul>
</li>
<li>GNBD でブロックデバイスをエクスポート

<ul>
<li>/dev/gnbd0, /dev/gnbd1, /dev/gnbd2, /dev/gnbd3 がネットワーク越しに見える</li>
</ul>
</li>
<li>クライアント側で GNBD でエクスポートされたブロックデバイスをインポート

<ul>
<li>/dev/gnbd0, /dev/gnbd1, /dev/gnbd2, /dev/gnbd3 をすべてインポート</li>
</ul>
</li>
<li>DM-MP により、2つのデバイスを1つに見せる

<ul>
<li>/dev/gnbd0 + /dev/gnbd1 が /dev/mapper/mpath0 として見える</li>
<li>/dev/gnbd2 + /dev/gnbd3 が /dev/mapper/mpath1 として見える</li>
</ul>
</li>
<li>CLVM により論理ボリュームを作成

<ul>
<li>/dev/mapper/mpath0 と /dev/mapper/mpath1 が物理ボリューム</li>
<li>この2つの物理ボリュームから、ボリュームグループ VG0 を作成</li>
<li>VG0 上に論理ボリューム LV0 を作成</li>
<li>この論理ボリュームが、/dev/VG0/LV0 として見える</li>
</ul>
</li>
<li>/dev/VG0/LV0 を GFS2 でフォーマット</li>
<li>ストレージ全体が論理ボリューム /dev/VG0/LV0 として見えるので、こいつをマウントする</li>
</ul>


<h1>構築手順概要</h1>

<p>構築手順はざっと以下のようになります。CentOS 5.2 + Xen で検証しています。</p>

<ul>
<li>Xen インスタンスをつくる

<ul>
<li>client0, client1</li>
<li>storage0a, storage0b</li>
<li>storage1a, storage1b</li>
</ul>
</li>
<li>ストレージサーバで GNBD によりエクスポートするブロックデバイスを作成する</li>
<li>2台セットで DRBD ミラーリング

<ul>
<li>storage0a と storage0b を同期</li>
<li>storage1a と storage1b を同期</li>
</ul>
</li>
<li>storage![01][ab] で GNBD によりブロックデバイスをエクスポート</li>
<li>client![01]でブロックデバイスをインポート</li>
<li>client![01]上で、DM-MP により複数のデバイスをまとめる

<ul>
<li>storage0[ab] からエクスポートしたデバイスをひとつに見せる</li>
<li>storage1[ab] からエクスポートしたデバイスをひとつに見せる</li>
</ul>
</li>
<li>DM-MP でまとめたデバイスを、CLVM によりひとつの論理ボリュームに見せる</li>
<li>論理ボリュームを GFS2 でフォーマットする</li>
<li>論理ボリュームをマウントする</li>
</ul>


<p>次回以降、それぞれについて解説していきます。</p>

<ul>
<li>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0]</li>
<li>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2]</li>
<li>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3]</li>
<li>[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4]</li>
<li>[wiki:ScalableStorageWithOSS05 OSS だけでスケーラブルなストレージを安価に構築する方法 #5]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS00/">ScalableStorageWithOSS00</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:21:10+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS00/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://trac.mizzy.org/public/wiki/HowToBuildAScalableStorageSystemWithOSS">TLUG Meeting 2008/09 で発表した How to build a scalable storage system with OSS</a> なんですが、発表では概要しか触れてなくて、じゃあいったいどうやって構築するのよ、という部分が全然ないので、ぼちぼちこのブログで書いていくことにします。</p>

<p>で、スケーラブルというだけだと曖昧なので、以下のような要件を満たすものを、スケーラブルなストレージと想定することにします。</p>

<ul>
<li>特殊なソフトウェアを必要とせずに、OS からファイルシステムとしてマウントできるもの。なので MogileFS、Hadoop Distributed File System、Google File System 等は対象外。（FUSE 使えばやれないこともないけど…）</li>
<li>容量をオンラインでダイナミックに追加できる。</li>
<li>SPOF がない。</li>
<li>複数のサーバから同時にマウントすることができる。</li>
<li>物理的に I/O を柔軟に分散することができる。</li>
<li>無駄な空きスペースができにくい。</li>
</ul>


<p>で、こういったストレージを、高価なハードウェアを買わずに、オープンソースソフトウェアだけで構築することができないか、と考えて、こんな感じで実現できそうだなあー、というのを発表したのが TLUG でのプレゼンの内容。このプレゼンで出てきたキーワードとその概要をおさらいすると、以下のような感じ。</p>

<ul>
<li><a href="http://sources.redhat.com/cluster/cman/">cman</a>

<ul>
<li>Cluster Manager</li>
<li>Red Hat Cluster Suite の一コンポーネント</li>
<li>クラスタのメンバーシップ管理とノード間のメッセージング</li>
<li>以下に出てくる、CLVM や GFS2 を使うために必要</li>
</ul>
</li>
<li><a href="http://sources.redhat.com/cluster/clvm/">CLVM</a>

<ul>
<li>Cluster Logical Volume Manager</li>
<li>LVM2 のクラスタ版</li>
<li>LVM2 のメタデータをクラスタノード間で自動的にシェアする</li>
<li>CLVM で作成された論理ボリュームは、すべてのクラスタノードから参照できる</li>
</ul>
</li>
<li><a href="http://sourceware.org/cluster/gnbd/">GNBD</a>

<ul>
<li>Global Network Block Device</li>
<li>TCP/IP ネットワーク経由でのブロックデバイスアクセス</li>
<li>iSCSI と同じようなもの</li>
<li>iSCSI との違いは、フェンシング機能を持っていること（フェンシングについては機会があれば説明します）</li>
</ul>
</li>
<li><a href="http://sources.redhat.com/cluster/gfs/">GFS2</a>

<ul>
<li>Global File System 2</li>
<li>クラスタファイルシステム</li>
<li>複数ノードから同時にマウントしてアクセスすることが可能</li>
<li>cman の DLM(Distributed Lock Manager) を利用して排他制御し、ファイルの整合性を保つ</li>
<li>OCFS(Oracle Cluster File System) なんかも同類</li>
</ul>
</li>
<li><a href="http://www.drbd.org/">DRBD</a>

<ul>
<li>Ditributed Replicated Block Device</li>
<li>要するにネットワーク越しの RAID1</li>
</ul>
</li>
<li><a href="http://www.redhat.com/docs/manuals/csgfs/browse/4.6/DM_Multipath/index.html">DM-MP</a>

<ul>
<li>Device-Mapper Multipath</li>
<li>複数の I/O 経路を仮想的にひとつに見せかけることができる</li>
<li>Active/Passive、Active/Active どちらでも対応可</li>
</ul>
</li>
</ul>


<p><a href="http://www.slideshare.net/mizzy/how-to-build-a-scalable-storage-system-at-tlug-meeting-20080913-presentation">TLUG での発表資料</a> では、それぞれを図で説明していたり、これらをどう組み合わせて利用するのかを説明しているので、ご参照ください。</p>

<p>で、これらを組み合わせて構築したストレージがとりあえず動くことは確認できたのだけど、実用に耐えうるかどうかは謎。特に以下の点が気になるところ。</p>

<ul>
<li>協調動作するコンポーネントが多くて、トラブルが起こりやすそう。</li>
<li>安定性。</li>
<li>パフォーマンス。</li>
<li>ストレージとなるサーバを追加して容量を増やした場合のオーバーヘッドの増加具合。</li>
</ul>


<p>なので、今後実際に使うかどうかはまだ決めかねてるのですが、一通り動かせるところまでは試してみたので、次回から実際の構築手順を書いていこうと思います。</p>

<h1>関連エントリ</h1>

<ul>
<li>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1]</li>
<li>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2]</li>
<li>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3]</li>
<li>[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4]</li>
<li>[wiki:ScalableStorageWithOSS05 OSS だけでスケーラブルなストレージを安価に構築する方法 #5]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/28/ScalableStorageWithOSS05/">ScalableStorageWithOSS05</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-28T17:20:40+09:00" pubdate>Dec 28<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/28/ScalableStorageWithOSS05/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>ずいぶん間があいちゃいましたが、[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4] のつづき。今回は DM-MP により束ねられた仮想的なブロックデバイス（/dev/mapper/mpath0 と /dev/mapper/mpath1）を、更に CLVM でひとつの論理ボリュームにし、GFS2 でフォーマットした後、実際にマウントしてみます。今回が最終回です。</p>

<h1>CLVM のインストールと設定</h1>

<p>client0, client1 双方でインストールと設定。</p>

<pre><code>$ sudo yum -y install lvm2-cluster
</code></pre>

<p>/etc/lvm/lvm.conf を編集し、locking_type を 3 に設定。</p>

<pre><code>locking_type = 3 
</code></pre>

<p>clvmd を起動。</p>

<pre><code>$ sudo /etc/init.d/clvmd start
</code></pre>

<h1>物理ボリューム/ボリュームグループ/論理ボリュームの作成</h1>

<p>通常の LVM と同様に、物理ボリューム/ボリュームグループ/論理ボリュームを作成。これは client0 上だけで行う。通常の LVM と異なるのは、vgcreate で &#8211;clustered y を指定すること。</p>

<pre><code>$ sudo /usr/sbin/pvcreate /dev/mapper/mpath0
$ sudo /usr/sbin/pvcreate /dev/mapper/mpath1
$ sudo /usr/sbin/vgcreate --clustered y VG0 /dev/mapper/mpath0 /dev/mapper/mpath1
$ sudo /usr/sbin/lvcreate --extent=510 --name=LV0 VG0 
</code></pre>

<p>pvdisplay, vgdisplay, lvdisplay で確認。</p>

<pre><code># pvdisplay
  --- Physical volume ---
  PV Name               /dev/dm-0
  VG Name               VG0
  PV Size               1023.90 MB / not usable 3.90 MB
  Allocatable           yes (but full)
  PE Size (KByte)       4096
  Total PE              255
  Free PE               0
  Allocated PE          255
  PV UUID               8Whtgb-7jrP-4M24-AMmB-aqA3-VQ8G-iSvijF

  --- Physical volume ---
  PV Name               /dev/dm-1
  VG Name               VG0
  PV Size               1023.90 MB / not usable 3.90 MB
  Allocatable           yes (but full)
  PE Size (KByte)       4096
  Total PE              255
  Free PE               0
  Allocated PE          255
  PV UUID               RC8J4S-hCRI-IlmJ-DqKB-Lk06-6Pza-nke5f8



# vgdisplay
  --- Volume group ---
  VG Name               VG0
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  2
  VG Access             read/write
  VG Status             resizable
  Clustered             yes
  Shared                no
  MAX LV                0
  Cur LV                1
  Open LV               1
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               1.99 GB
  PE Size               4.00 MB
  Total PE              510
  Alloc PE / Size       510 / 1.99 GB
  Free  PE / Size       0 / 0
  VG UUID               XPwKeu-5lDP-9eCv-gUoC-HIB0-ItMV-4bxRHN 



sudo /usr/sbin/lvdisplay
  --- Logical volume ---
  LV Name                /dev/VG0/LV0
  VG Name                VG0
  LV UUID                PYScA4-OeAf-Dndj-5hMW-4a0x-Yfaj-6zIYpE
  LV Write Access        read/write
  LV Status              available
  # open                 0
  LV Size                1.99 GB
  Current LE             510
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:2  
</code></pre>

<p>client1 でも lvdisplay で論理ボリュームが見えるかを確認。</p>

<pre><code>sudo /usr/sbin/lvdisplay
  --- Logical volume ---
  LV Name                /dev/VG0/LV0
  VG Name                VG0
  LV UUID                PYScA4-OeAf-Dndj-5hMW-4a0x-Yfaj-6zIYpE
  LV Write Access        read/write
  LV Status              available
  # open                 0
  LV Size                1.99 GB
  Current LE             510
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:2  
</code></pre>

<h1>GFS2でフォーマットしてマウント</h1>

<p>client0, client1 両方に gfs2-utils パッケージをインストール。</p>

<pre><code>$ sudo yum -y install gfs2-utils
</code></pre>

<p>client0 上だけで、mkfs2.gfs2 を実行してフォーマットする。</p>

<pre><code>$ sudo /sbin/mkfs.gfs2 -p lock_dlm -t client:gfs2 -j 4 /dev/VG0/LV0
This will destroy any data on /dev/VG0/LV0.

Are you sure you want to proceed? [y/n] y 

Device:                    /dev/VG0/LV0
Blocksize:                 4096
Device Size                1.99 GB (522240 blocks)
Filesystem Size:           1.99 GB (522240 blocks)
Journals:                  4
Resource Groups:           8
Locking Protocol:          "lock_dlm"
Lock Table:                "client:gfs2"
</code></pre>

<p>client0, client1 両方でマウント。</p>

<pre><code>$ sudo mount /dev/VG0/LV0 /mnt  
</code></pre>

<p>client0 上でファイルをつくると、client1 でも見えることを確認。</p>

<pre><code>client0$ sudo touch /mnt/ieiri



client1$ ls /mnt
ieiri
</code></pre>

<p>これで完了。現在の状態を図にすると、以下のような感じで、4台のサーバに分散されたブロックデバイスがひとつの論理ボリュームとして見え、更に複数のマシンから同時にマウントできている。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/ScalableStorageWithOSS01/scalable_storage.jpg?format=raw)]]</p>

<h1>最後に</h1>

<p>本当はここから更に、ダイナミックにストレージサーバを追加して容量を拡張する手順なんかも書きたいところなんですが、[wiki:ScalableStorageWithOSS00 #0] でも書いたような、以下の懸念点があるため、今のところ実用で使おうとは考えていません。</p>

<ul>
<li>協調動作するコンポーネントが多くて、トラブルが起こりやすそう。

<ul>
<li>実際に各コンポーネントを起動/停止する順を間違えると、ハングアップしたりする。</li>
<li>低レイヤーなだけに、トラブルが起きたときの対応がとても難しそう。</li>
</ul>
</li>
<li>安定性。</li>
<li>パフォーマンス。</li>
<li>ストレージとなるサーバを追加して容量を増やした場合のオーバーヘッドの増加具合。</li>
</ul>


<p>というわけで、これ以上踏む込むことは今のところやめておきます。まずはこんな技術があるんだ、ということを知って、動くところまで試しただけでも、自分にとっては十分収穫があったな、と。</p>

<h1>関連エントリ</h1>

<ul>
<li>[wiki:ScalableStorageWithOSS00 OSS だけでスケーラブルなストレージを安価に構築する方法 #0]</li>
<li>[wiki:ScalableStorageWithOSS01 OSS だけでスケーラブルなストレージを安価に構築する方法 #1]</li>
<li>[wiki:ScalableStorageWithOSS02 OSS だけでスケーラブルなストレージを安価に構築する方法 #2]</li>
<li>[wiki:ScalableStorageWithOSS03 OSS だけでスケーラブルなストレージを安価に構築する方法 #3]</li>
<li>[wiki:ScalableStorageWithOSS04 OSS だけでスケーラブルなストレージを安価に構築する方法 #4]</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/12/07/InfraEngineerPanelDiscussionFinished/">InfraEngineerPanelDiscussionFinished</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-12-07T21:59:33+09:00" pubdate>Dec 7<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/12/07/InfraEngineerPanelDiscussionFinished/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.pasonatech.co.jp/event/index.jsp?no=1137">パソナテック10周年記念　PTカンファレンスVol.6 インフラエンジニア討論会2008 ～インフラエンジニア進化論～</a> でパネリストとして登壇してきました。</p>

<p>ドクターペッパーは出ませんでした。（と <a href="http://journal.soffritto.org/">lopnor さん</a> から指摘されて、そういえば、と思った。）</p>

<p>思ったこととかは後で書く。（と言っておいて書かないと思う、たぶん。）</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/11/27/DrPepperRanking20081127/">DrPepperRanking20081127</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-11-27T01:39:44+09:00" pubdate>Nov 27<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/11/27/DrPepperRanking20081127/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://journal.soffritto.org/">lopnor さん</a> からランキング漏れを指摘されたので、naoya さんの分も合わせて更新。</p>

<p>|| 1 || <a href="http://d.hatena.ne.jp/naoya/">naoya さん</a> || 240本 ||
|| 2 || <a href="http://d.hatena.ne.jp/hirose31/">ひろせさん</a> || 25本 ||
|| 3 || <a href="http://blog.hbkr.jp/">弊社代表取締役社長</a>   || 10本 ||
|| 4 || Xen Summit 2008 Tokyo|| 3本（フリードリンクとして提供されたものからいただいた数） ||
|| 5 || <a href="http://journal.soffritto.org/">lopnor さん</a> || 2本（<a href="http://soozy.org/index.cgi?CodereposCon1">CodeReposCon #1</a> で、株式会社ソフリットの経費から） ||
|| 6 || <a href="http://overlasting.dyndns.org/">overlastさん</a>|| 1本（たしか SoozyCon かな？いまはなきアジトでいただいた） ||
|| 7 || <a href="http://d.hatena.ne.jp/hiboma/">id:hiboma</a> || 0.5本（YAPC::Asia Tokyo 2008 でプレゼン時のドリンクとして） ||
|| 7 || <a href="http://kayano.jugem.cc/">みるくぜりーさん</a> || 0.5本（同じく YAPC::Asia Tokyo 2008 で、id:hiboma とともに持ってきてくれた）||
|| 9 || 弊社サーバエンジニアのまーくん || 2本（ただしドクターペッパーではなくルートビア。あれ、ドクペももらったことあったっけ？）
|| 10 || <a href="http://d.hatena.ne.jp/yappo/">Yappo さん</a> || 1本（ただしドクターペッパーではなくピンクペッパー） ||</p>

<p>まだ漏れがあるような気がするので、お気づきの方はご連絡ください。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/11/26/DrPepperFromNaoya/">DrPepperFromNaoya</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-11-26T01:22:26+09:00" pubdate>Nov 26<span>th</span>, 2008</time>
        
         | <a href="/blog/2008/11/26/DrPepperFromNaoya/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><a href="http://d.hatena.ne.jp/naoya/">はてな CTO の naoya さん</a> からもお祝いをいただきました。しかも10箱（240缶）も。naoya さん、ありがとうございます！</p>

<p>240缶のドクペを前に満足げな様子。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromNaoya/00.jpg?format=raw)]]</p>

<p>なんとなくポーズをとってみる。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromNaoya/01.jpg?format=raw)]]</p>

<p>今回も <a href="http://pplog.jugem.cc/">pplog さん</a> が撮影だけじゃなく、ネタ画像つくってくれました。いつもありがとうございます！</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromNaoya/02.jpg?format=raw)]]</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2008/11/22/DrPepperFromHirose/">DrPepperFromHirose</a></h1>
    
    
      <p class="meta">
        




  

<time datetime="2008-11-22T14:43:24+09:00" pubdate>Nov 22<span>nd</span>, 2008</time>
        
         | <a href="/blog/2008/11/22/DrPepperFromHirose/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>ペパボの上場承認のお祝い、ということで、<a href="http://d.hatena.ne.jp/hirose31/">ひろせさん</a> よりドクターペッパーを頂きました。</p>

<p>朝会社に来てみると、やけにでかい Amazon の段ボール箱が机の前にあり、空けてみたところこんな物体が入ってました。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromHirose/00.jpg?format=raw)]]</p>

<p>大きさを比較するために自分で持ってみたところ。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromHirose/01.jpg?format=raw)]]</p>

<p>ラッピングを外してみると。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromHirose/02.jpg?format=raw)]]</p>

<p>箱の中身。</p>

<p>[[Image(http://trac.mizzy.org/public/attachment/wiki/DrPepperFromHirose/03.jpg?format=raw)]]</p>

<p>というわけで、僕へのドクターペッパープレゼント数ランキングでは、今までトップであった<a href="http://blog.hbkr.jp/">弊社代表取締役社長家入</a>（10本）を軽々と突破し、ひろせさんが25本でダントツの一位となりました。（今回の24本＋KLab勉強会での1本。）</p>

<p>ちなみに、現在のドクターペッパープレゼントランキングは下記のとおり。</p>

<p>|| 1 || ひろせさん || 25本 ||
|| 2 || 弊社代表取締役社長   || 10本 ||
|| 3 || Xen Summit 2008 Tokyo|| 3本（フリードリンクとして提供されたものからいただいた数） ||
|| 4 || <a href="http://overlasting.dyndns.org/">overlastさん</a>|| 1本（たしか SoozyCon かな？いまはなきアジトでいただいた） ||
|| 5 || <a href="http://d.hatena.ne.jp/hiboma/">id:hiboma</a> || 0.5本（YAPC::Asia Tokyo 2008 でプレゼン時のドリンクとして） ||
|| 5 || <a href="http://kayano.jugem.cc/">みるくぜりーさん</a> || 0.5本（同じく YAPC::Asia Tokyo 2008 で、id:hiboma とともに持ってきてくれた）||
|| 7 || 弊社サーバエンジニアのまーくん || 2本（ただしドクターペッパーではなくルートビア。あれ、ドクペももらったことあったっけ？）
|| 8 || <a href="http://d.hatena.ne.jp/yappo/">Yappo さん</a> || 1本（ただしドクターペッパーではなくピンクペッパー） ||</p>

<p>「あれ、俺プレゼントしたのにランキングに載ってないよ？」という方がいらっしゃいましたら、ご連絡ください。</p>

<p>また、写真撮影してくれた pplog さんが、今回のひろせさんからのプレゼントを記念して、<a href="http://pplog.jugem.cc/?eid=885">素敵な画像を作成してくれました</a>。</p>

<p>[[Image(http://pplog.img.jugem.jp/20081121_592213_t.jpg)]]</p>

<p>ちなみに、中段の「飲みきれないドクペのために 今、何ができるか」は Yappo さんのアイデア。</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/9/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/7/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/01/06/weechat-script-for-pushing-notification-to-im-dot-kayac-dot-com/">WeeChat script for pushing notification to im.kayac.com</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/12/20/lunar-eclipse/">Lunar Eclipse</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/12/20/a-picture-of-geminids/">A Picture of Geminids</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/12/13/maglica-presentation-at-hatena/">Maglica Presentation At Hatena</a>
      </li>
    
      <li class="post">
        <a href="/blog/2011/11/15/octopress-on-heteml/">How to deploy a blog made by Octopress to Heteml</a>
      </li>
    
  </ul>
</section>




  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Gosuke Miyashita -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'mizzyorg';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
